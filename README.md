# Signaling-Data-ETL
### Overview
This project focuses on building a robust ETL (Extract, Transform, Load) pipeline using SQL Server Integration Services (SSIS) to process and audit signaling data generated by network subscribers. The ETL pipeline is designed to efficiently handle large volumes of data, apply error handling techniques, and maintain a detailed audit trail for data processing.

## Project Structure
### ETL Pipeline
- Source Data: Subscriber signaling data, including activities like calls, browsing, and SMS, collected by the network vendor in real-time and in bulk.
- Processing: The pipeline processes dumped files, handles errors, and maintains an audit of processed records.
![image](https://github.com/user-attachments/assets/bd81e6c4-dc16-4cd4-9df8-65a94ba0d398)
![image](https://github.com/user-attachments/assets/84ffe08f-f3fb-4169-9c6b-e7c51d92281c)

### Audit Table
dim_audit: Captures the number of rows extracted, inserted, and rejected for each file, offering a comprehensive overview of the ETL process.
### Error Handling
- Implemented various error-handling techniques to manage:
  - Null values in non-nullable attributes.
  - Data type mismatches.
  - Incomplete records.
## Technical Details
- Source Systems: Flat files, dynamically processed through the Foreach Loop container.
- Variables: Used to dynamically adjust source system paths.
- Audit Mechanism: Tracks key metrics like rows processed and records errors to ensure data quality.
## Usage
To use this project, follow these steps:

1- Clone the repository to your local machine.
2- Open the SSIS project in Visual Studio.
3- Configure the connection managers to point to your source systems.
4- Run the ETL packages to load and audit the signaling data.
License
License: MIT

## Acknowledgments:
This is one of Garage eduacation's use cases, and i used Et3lm Online's implementation ofit as a reference for me.
This project was developed using SQL Server Integration Services (SSIS) and real-world signaling data processing scenarios.

